{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import features as cf\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractores simples\n",
    "Extraen atributos bivalentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_extractors():\n",
    "    # Extraigo dos atributos simples: \n",
    "    # 1) Longitud del mail.\n",
    "    # 2) Cantidad de espacios en el mail.\n",
    "    # 3) Tiene el mail contenido HTML?\n",
    "    # 4) Tiene el mail imágenes?\n",
    "    # 5) Cantidad de oraciones\n",
    "    \n",
    "    return [ ('body_length', cf.body_length), \n",
    "      ('count_spaces', cf.count_spaces), \n",
    "      ('has_html', cf.has_html), \n",
    "      ('has_image', cf.has_image), \n",
    "      ('number_of_sentences', cf.number_of_sentences) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer_extractor(vectorizer_type, **kwargs):\n",
    "    if vectorizer_type == \"bow\":\n",
    "        vectorizer = CountVectorizer(stop_words='english', **kwargs)\n",
    "    elif vectorizer_type == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', **kwargs)\n",
    "    elif vectorizer_type == \"hashing_bow\":\n",
    "        vectorizer = HashingVectorizer(stop_words='english', **kwargs)\n",
    "    else:\n",
    "        raise ValueError('Invalid vectorizer_type. Expected \\'bow\\', \\'tfidf\\' or \\'hashing_bow\\'')\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline de extracción\n",
    "Cadena de extracción de atributos.\n",
    "\n",
    "En la celda inferior, se utiliza body_and_subject_vectorizer para determinar si el sujeto y el cuerpo del mail deben estar juntos en la creación de la matriz DF-IDF (la extracción se realiza sobre el texto concatenado) o separados (se realiza la extracción sobre los dos textos independientemente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def features_extractors(simple_features=True, subject_vectorizer='tfidf', body_vectorizer='tfidf', body_and_subject_vectorizer=None):\n",
    "    extractors = []\n",
    "    if simple_features:\n",
    "        # Simple features extactor\n",
    "        extractors = [('simple_features', cf.SimpleFeaturesExtractor(simple_extractors()))]\n",
    "    \n",
    "    if body_and_subject_vectorizer is not None:\n",
    "        # Pipeline for pulling vectorizer features from the post's body\n",
    "        extractors = extractors + \\\n",
    "            [('body_and_subject', Pipeline([\n",
    "                ('selector', ColumnSelectorExtractor('body_and_subject')),\n",
    "                (body_vectorizer, vectorizer_extractor(body_and_subject_vectorizer)),\n",
    "            ]))]\n",
    "    else:\n",
    "        if subject_vectorizer is not None:\n",
    "            # Pipeline for pulling vectorizer features from the post's subject\n",
    "            extractors = extractors + \\\n",
    "                [('subject', Pipeline([\n",
    "                    ('selector', ColumnSelectorExtractor('subject')),\n",
    "                    (subject_vectorizer, vectorizer_extractor(subject_vectorizer)),\n",
    "                ]))]\n",
    "\n",
    "        if body_vectorizer is not None:\n",
    "            # Pipeline for pulling vectorizer features from the post's body\n",
    "            extractors = extractors + \\\n",
    "                [('body', Pipeline([\n",
    "                    ('selector', ColumnSelectorExtractor('body')),\n",
    "                    (body_vectorizer, vectorizer_extractor(body_vectorizer)),\n",
    "                ]))]\n",
    "    \n",
    "    # Use FeatureUnion to combine the features\n",
    "    return FeatureUnion(extractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset/ham_dev.json\n",
      "Done in 3.903567s\n",
      "Loaded 45000(465.272MB) mails\n",
      "Parsing mails\n",
      "Done in 19.629904s\n",
      "Parsed 45000 mails\n",
      "Loading data from dataset/spam_dev.json\n",
      "Done in 2.253287s\n",
      "Loaded 45000(200.517MB) mails\n",
      "Parsing mails\n",
      "Done in 18.128129s\n",
      "Parsed 45000 mails\n",
      "Generating Pandas DataFrame\n",
      "Done in 64.343704s\n",
      "Splitting into Training and Test Set\n",
      "Done in 64.416754s\n",
      "Train Set: 72000 samples - Ham: 35942(0.50%) Spam: 36058(0.50%)\n",
      "Test Set:  18000 samples - Ham: 9058(0.50%) Spam: 8942(0.50%)\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = load_data(merge_body_and_subject=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentación con árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuerpos y sujetos concatenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_pipeline_unified_body_and_subject = Pipeline([\n",
    "  ('features_extractor', features_extractors(True, body_and_subject_vectorizer='tfidf')),\n",
    "  ('tree_classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "result = cross_val_score(dt_pipeline_unified_body_and_subject, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.978764 STD: 0.002119\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: %f STD: %f\" % (np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuerpos y sujetos separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_pipeline_splitted_body_and_subject = Pipeline([\n",
    "  ('features_extractor', features_extractors(True, 'tfidf', 'tfidf')),\n",
    "  ('tree_classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "result = cross_val_score(dt_pipeline_splitted_body_and_subject, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.979306 STD: 0.002468\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: %f STD: %f\" % (np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentación con SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuerpos y sujetos concatenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_pipeline_unified_body_and_subject = Pipeline([\n",
    "  ('features_extractor', features_extractors(True, body_and_subject_vectorizer='tfidf')),\n",
    "  ('svm_classifier', SVC())\n",
    "])\n",
    "\n",
    "result = cross_val_score(svm_pipeline_unified_body_and_subject, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.610222 STD: 0.002689\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: %f STD: %f\" % (np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuerpos y sujetos separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_pipeline_splitted_body_and_subject = Pipeline([\n",
    "  ('features_extractor', features_extractors(True, 'tfidf', 'tfidf')),\n",
    "  ('svm_classifier', SVC())\n",
    "])\n",
    "\n",
    "result = cross_val_score(svm_pipeline_splitted_body_and_subject, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.608903 STD: 0.002846\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: %f STD: %f\" % (np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentación con Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive bayes no funciona optimamente con datos esparsos. Sumado a esta limitación del algorimto, la conversión de nuestras matrices a esparsar implica un consumo de memoria demasiado alto, imposibilitando la utilización de este algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuerpos y sujetos concatenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnb_pipeline_unified_body_and_subject = Pipeline([\n",
    "  ('features_extractor', features_extractors(True, body_and_subject_vectorizer='tfidf')),\n",
    "  ('mnb_classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "result = cross_val_score(mnb_pipeline_unified_body_and_subject, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.912722 STD: 0.005642\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: %f STD: %f\" % (np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuerpos y sujetos separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_pipeline_splitted_body_and_subject = Pipeline([\n",
    "  ('features_extractor', features_extractors(True, 'tfidf', 'tfidf')),\n",
    "  ('mnb_classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "result = cross_val_score(mnb_pipeline_splitted_body_and_subject, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.921708 STD: 0.005261\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: %f STD: %f\" % (np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuerpos y sujetos concatenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnb_pipeline_unified_body_and_subject = Pipeline([\n",
    "  ('features_extractor', features_extractors(True, body_and_subject_vectorizer='tfidf')),\n",
    "  ('bnb_classifier', BernoulliNB())\n",
    "])\n",
    "\n",
    "result = cross_val_score(bnb_pipeline_unified_body_and_subject, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.948569 STD: 0.002133\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: %f STD: %f\" % (np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuerpos y sujetos separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnb_pipeline_splitted_body_and_subject = Pipeline([\n",
    "  ('features_extractor', features_extractors(True, 'tfidf', 'tfidf')),\n",
    "  ('bnb_classifier', BernoulliNB())\n",
    "])\n",
    "\n",
    "result = cross_val_score(bnb_pipeline_splitted_body_and_subject, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.951486 STD: 0.002426\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: %f STD: %f\" % (np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentación con Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuerpos y sujetos concatenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rndf_pipeline_unified_body_and_subject = Pipeline([\n",
    "  ('features_extractor', features_extractors(True, body_and_subject_vectorizer='tfidf')),\n",
    "  ('rndf_classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "result = cross_val_score(rndf_pipeline_unified_body_and_subject, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.978708 STD: 0.001738\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: %f STD: %f\" % (np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuerpos y sujetos separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rndf_pipeline_splitted_body_and_subject = Pipeline([\n",
    "  ('features_extractor', features_extractors(True, 'tfidf', 'tfidf')),\n",
    "  ('rndf_classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "result = cross_val_score(rndf_pipeline_splitted_body_and_subject, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.979264 STD: 0.002180\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: %f STD: %f\" % (np.mean(result), np.std(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
