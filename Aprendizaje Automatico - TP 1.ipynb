{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Automatico - TP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Development\\Anaconda2\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import features as cf\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes features componen el conjunto de features simples(?) a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_extractors():\n",
    "    # Extraigo dos atributos simples: \n",
    "    # 1) Longitud del mail.\n",
    "    # 2) Cantidad de espacios en el mail.\n",
    "    # 3) Tiene el mail contenido HTML?\n",
    "    # 4) Tiene el mail imágenes?\n",
    "    # 5) Cantidad de oraciones\n",
    "    \n",
    "    return [ ('body_length', cf.body_length), \n",
    "      ('count_spaces', cf.count_spaces), \n",
    "      ('has_html', cf.has_html), \n",
    "      ('has_image', cf.has_image), \n",
    "      ('number_of_sentences', cf.number_of_sentences) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer_extractor(vectorizer_type, **kwargs):\n",
    "    if vectorizer_type == \"bow\":\n",
    "        vectorizer = CountVectorizer(stop_words='english', **kwargs)\n",
    "    elif vectorizer_type == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', **kwargs)\n",
    "    elif vectorizer_type == \"hashing_bow\":\n",
    "        vectorizer = HashingVectorizer(stop_words='english', **kwargs)\n",
    "    else:\n",
    "        raise ValueError('Invalid vectorizer_type. Expected \\'bow\\', \\'tfidf\\' or \\'hashing_bow\\'')\n",
    "    \n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos\n",
    "Cargamos y spliteamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset/ham_dev.json\n",
      "Done in 2.500000s\n",
      "Loaded 45000(465.272MB) mails\n",
      "Parsing mails\n",
      "Done in 12.578000s\n",
      "Parsed 45000 mails\n",
      "Loading data from dataset/spam_dev.json\n",
      "Done in 1.525000s\n",
      "Loaded 45000(200.517MB) mails\n",
      "Parsing mails\n",
      "Done in 12.219000s\n",
      "Parsed 45000 mails\n",
      "Generating Pandas DataFrame\n",
      "Done in 22.020000s\n",
      "Splitting into Training and Test Set\n",
      "Done in 22.051000s\n",
      "Train Set: 72000 samples - Ham: 35978(0.50%) Spam: 36022(0.50%)\n",
      "Test Set:  18000 samples - Ham: 9022(0.50%) Spam: 8978(0.50%)\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de atributos\n",
    "\n",
    "A continuación, definimos nuestro pipeline para la extracción de features.\n",
    "1. Se realiza la extracción de las simple features descriptas anteriormente.\n",
    "2. Se computa la matriz de term frequency–inverse document frequency para:\n",
    "    - El sujeto de los mails.\n",
    "    - El cuerpo de los mails.\n",
    "3. Se utiliza el sentiment analyzer de NLTK para extraer la intención del mensaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def features_extractors(simple_features=True, subject_vectorizer='tfidf', body_vectorizer='tfidf'):\n",
    "    extractors = []\n",
    "    if simple_features:\n",
    "        # Simple features extactor\n",
    "        extractors = [('simple_features', cf.SimpleFeaturesExtractor(simple_extractors()))]\n",
    "    \n",
    "    if subject_vectorizer is not None:\n",
    "        # Pipeline for pulling vectorizer features from the post's subject\n",
    "        extractors = extractors + \\\n",
    "            [('subject', Pipeline([\n",
    "                ('selector', ColumnSelectorExtractor('subject')),\n",
    "                (subject_vectorizer, vectorizer_extractor(subject_vectorizer)),\n",
    "            ]))]\n",
    "            \n",
    "    if body_vectorizer is not None:\n",
    "        # Pipeline for pulling vectorizer features from the post's body\n",
    "        extractors = extractors + \\\n",
    "            [('body', Pipeline([\n",
    "                ('selector', ColumnSelectorExtractor('body')),\n",
    "                (body_vectorizer, vectorizer_extractor(body_vectorizer)),\n",
    "            ]))]\n",
    "    \n",
    "    # Use FeatureUnion to combine the features\n",
    "    return FeatureUnion(extractors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de clasificadores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árbol de decisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters_grid = {\n",
    "    'features_extractor__subject__tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'features_extractor__subject__tfidf__max_df': [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0],\n",
    "    'features_extractor__subject__tfidf__min_df': [0.0, 0.001, 0.01, 0.05, 0.1, 0.5, 0.75, 0.9], \n",
    "    'features_extractor__subject__tfidf__binary': [False, True],\n",
    "    'features_extractor__subject__tfidf__sublinear_tf': [False, True],\n",
    "    'features_extractor__body__tfidf__ngram_range': [(1, 1), (1, 2), (2, 2)],\n",
    "    'features_extractor__body__tfidf__max_df': [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0],\n",
    "    'features_extractor__body__tfidf__min_df': [0.0, 0.001, 0.01, 0.05, 0.1, 0.5, 0.75, 0.9], \n",
    "    'features_extractor__body__tfidf__binary': [False, True],\n",
    "    'features_extractor__body__tfidf__sublinear_tf': [False, True],\n",
    "    'tree_classifier__criterion': ['gini', 'entropy'],\n",
    "    'tree_classifier__max_features': ['sqrt', 'log2', 0.5, None],\n",
    "    'tree_classifier__max_depth': [3, 5, 10, None],\n",
    "    'tree_classifier__min_samples_split': [1, 3, 5, 10],\n",
    "    'tree_classifier__min_samples_leaf': [1, 3, 5, 10]    \n",
    "}\n",
    "\n",
    "dt = GridSearchCV(Pipeline([\n",
    "  ('features_extractor', features_extractors()),\n",
    "  ('tree_classifier', DecisionTreeClassifier())\n",
    "]), parameters_grid, cv=10, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 231211008 candidates, totalling 2312110080 fits\n"
     ]
    }
   ],
   "source": [
    "dt.fit(train_set, train_set['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
