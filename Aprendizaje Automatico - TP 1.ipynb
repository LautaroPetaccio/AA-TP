{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje Automatico - TP 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Software\\Development\\Anaconda2\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import features as cf\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes features componen el conjunto de features simples(?) a utilizar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_extractors():\n",
    "    # Extraigo dos atributos simples: \n",
    "    # 1) Longitud del mail.\n",
    "    # 2) Cantidad de espacios en el mail.\n",
    "    # 3) Tiene el mail contenido HTML?\n",
    "    # 4) Tiene el mail imágenes?\n",
    "    # 5) Cantidad de oraciones\n",
    "    \n",
    "    return [ ('body_length', cf.body_length), \n",
    "      ('count_spaces', cf.count_spaces), \n",
    "      ('has_html', cf.has_html), \n",
    "      ('has_image', cf.has_image), \n",
    "      ('number_of_sentences', cf.number_of_sentences) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos\n",
    "Cargamos y spliteamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from dataset/ham_dev.json\n",
      "Done in 5.484000s\n",
      "Loaded 45000(465.272MB) mails\n",
      "Parsing mails\n",
      "Done in 0.125000s\n",
      "Parsed 500 mails\n",
      "Loading data from dataset/spam_dev.json\n",
      "Done in 3.206000s\n",
      "Loaded 45000(200.517MB) mails\n",
      "Parsing mails\n",
      "Done in 0.200000s\n",
      "Parsed 500 mails\n",
      "Generating Pandas DataFrame\n",
      "Done in 0.531000s\n",
      "Splitting into Training and Test Set\n",
      "Done in 0.535000s\n",
      "Train Set: 800 samples - Ham: 401(0.50%) Spam: 399(0.50%)\n",
      "Test Set:  200 samples - Ham: 99(0.49%) Spam: 101(0.51%)\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de atributos\n",
    "\n",
    "A continuación, definimos nuestro pipeline para la extracción de features.\n",
    "1. Se realiza la extracción de las simple features descriptas anteriormente.\n",
    "2. Se computa la matriz de term frequency–inverse document frequency para:\n",
    "    - El sujeto de los mails.\n",
    "    - El cuerpo de los mails.\n",
    "3. Se utiliza el sentiment analyzer de NLTK para extraer la intención del mensaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_extractor = FeatureUnion(\n",
    "    # Use FeatureUnion to combine the features\n",
    "    [\n",
    "        # Simple features extactor\n",
    "        ('simple_features', cf.SimpleFeaturesExtractor(simple_extractors())),\n",
    "\n",
    "        # Pipeline for pulling features from the post's subject\n",
    "        ('subject', Pipeline([\n",
    "            ('selector', ColumnSelectorExtractor('subject')),\n",
    "            ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "        ])),\n",
    "\n",
    "        # Pipeline for pulling features from the post's body\n",
    "        ('subject', Pipeline([\n",
    "            ('selector', ColumnSelectorExtractor('body')),\n",
    "            ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "        ])),\n",
    "\n",
    "        # Pipeline for the sentiment analysis feature\n",
    "        ('sentiment_analysis', Pipeline([\n",
    "            ('selector', ColumnSelectorExtractor('body')),\n",
    "            ('stats', cf.SentimentsStats()),\n",
    "            ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "        ]))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de clasificadores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árbol de decisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt_pipeline = Pipeline([\n",
    "  ('features_extractor', features_extractor),\n",
    "  ('tree_classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "cross_val_score(dt_pipeline, train_set, train_set['label'], cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
